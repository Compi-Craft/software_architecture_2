# Homework 2

## Task 1:
Hazelcast та Management Center скачав з посилання яке було в завданні
## Task 2:
Ноди запускав у wsl, відкрив паралельно три термінали, отримав кластер з трьох учасників
![Alt Text](./images/image.png)
## Task 3:
З допомогою коду у task3.py під'єднався як кліент до кластера та заповнив даними. Розподіл даних між нодами
![Alt Text](./images/image_2.png)
Ситуація після відключення однієї ноди:
![Alt Text](./images/image_3.png)
Ситуація після відключення двох нод послідовно:
![Alt Text](./images/image_4.png)
Після двох нод одночасно
![Alt Text](./images/image_5.png)
![Alt Text](./images/image_6.png)
Є втрата даних. Для запобігання цього можна використати різні механізми реплікації та відновлення даних за рахунок бекапів
## Task 4:
Запустив паралельно збільшення одного значення використовуючи потоки <br>
![Alt Text](./images/image_7.png)
<br>
Багато даних "губиться" через race condition
## Task 5:
Результат <br>
![Alt Text](./images/image_8.png)
## Task 6:
Результат <br>
![Alt Text](./images/image_9.png)
## Task 7:
З блокування дійсно зникає race condition. По швидкості виконання обидва способи у мене фактично одинакові з скріншотів вище
## Task 8:
Результати <br>
![Alt Text](./images/image_10.png)
Вичитування відбувається непослідовно
Якщо забрати читачів, то черга просто заповниться до 10-ти і стане в очікування <br>
![Alt Text](./images/image_1.png)